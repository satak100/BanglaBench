Authors' implementation of **Too Late to Train, Too Early To Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs**

### ðŸ“„ [Paper Link](https://arxiv.org/pdf/2407.00416)  
# BanglaBench
BanglaBench explores the necessity of Bengali-specific large language models (LLMs) by benchmarking open-weight and closed-source LLMs like LLaMA-3 and GPT-4 against fine-tuned encoder-decoder models on diverse Bengali NLP tasks, including translation, summarization, and question-answering. The findings highlight key challenges such as inefficient tokenization of Bengali script and biases in machine-translated datasets, emphasizing the urgent need for a dedicated Bengali LLM backed by high-quality pretraining and instruction-tuning datasets.

### Citation  

If you find BanglaBench helpful in your research, please cite our conference paper:  

```bibtex  
@inproceedings{BanglaBench2025,  
  title={Too Late to Train, Too Early to Use? A Study on Necessity and Viability of Low-Resource Bengali LLMs},  
  author={Tamzeed Mahfuz and Satak Kumar Dey and Ruwad Naswan and Hasnaen Adil and Khondker Salman Sayeed and Haz Sameen Shahgir},  
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics (COLING)},  
  year={2025},  
  organization={ACL},  
  url={#}  
} 
